---
title: "Stats Final Project"
author: "Lenka and Nina"
date: "2022-12-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(fastDummies)
library(glmnet)
library(broom)

library(pscl)
```

```{r}
#functions 
lasso.bic <- function(y,x,extended=FALSE) {
  #Select model in LASSO path with best BIC (using LASSO regression estimates)
  #Input
  # - y: vector with response variable
  # - x: design matrix
  #
  #Output: list with the following elements
  # - coef: LASSO-estimated regression coefficient with lambda set via BIC
  # - ypred: predicted y
  # - lambda.opt: optimal value of lambda
  # - lambda: data.frame with bic and number of selected variables for each value of lambda
  require(glmnet)
  fit <- glmnet(x=x,y=y,family='binomial',alpha=1)
  pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
  n <- length(y)
  p <- colSums(fit$beta!=0) + 1
  if (!extended){
    bic <- deviance(fit) + log(n)*p 
  } else {
    bic <- deviance(fit) + log(n)*p + 2*log(choose(ncol(x),p))
  }

  sel <- which.min(bic)
  beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
  ypred <- pred[,sel]
  ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
  return(ans)
}
kfoldCV.lasso <- function(y,x,K=10,seed,criterion='cv') {
## Perform K-fold cross-validation for LASSO regression estimate (lambda set either via cross-val or BIC or EBIC)
## Input
## - y: response
## - x: data.frame with predictors, intercept should not be present
## - K: number of folds in K-fold cross-validation
## - seed: random number generator seed (optional)
## - criterion: the criterion to select the penalization parameter, either cross-val or BIC or EBIC
## Output
## - pred: cross-validated predictions for y
## - ssr: residual sum of squares, sum((y-pred)^2)
  require(glmnet)
  if (!missing(seed)) set.seed(seed)
  subset <- rep(1:K,ceiling(nrow(x)/K))[1:nrow(x)]
  subset <- sample(subset,size=nrow(x),replace=FALSE)
  pred <- double(nrow(x))
  cat("Starting cross-validation")
  if (ncol(x)>0) {  #if there are some covariates
    for (k in 1:K) {
        sel <- subset==k
        if (criterion=='cv') {
            fit <- cv.glmnet(x=x[!sel,,drop=FALSE], y=y[!sel], alpha = 1, nfolds=10,family = 'binomial',type_measure='auc')
            b= as.vector(coef(fit,s='lambda.min'))
            pred[sel] <- b[1] + x[sel,,drop=FALSE] %*% as.matrix(b[-1])
        } else if (criterion=='bic'){
            fit <- lasso.bic(y=y[!sel],x=x[!sel,,drop=FALSE])
            pred[sel] <- fit$coef[1] + x[sel,,drop=FALSE] %*% matrix(fit$coef[-1],ncol=1)
        } else if (criterion=='ebic'){
            fit <- lasso.bic(y=y[!sel],x=x[!sel,,drop=FALSE],extended = TRUE)
            pred[sel] <- fit$coef[1] + x[sel,,drop=FALSE] %*% matrix(fit$coef[-1],ncol=1) 
        } else { stop("method.lambda not implemented") }
        cat(".")
    }
  } else { #if there are no covariates, just use the intercept
    for (k in 1:K) {
      sel <- subset==k
      pred[sel] <- mean(y[!sel],na.rm=TRUE)
    }
  }
  cat("\n")
  return(list(pred=pred,ssr=sum((pred-y)^2,na.rm=TRUE)))
}
```

# Recidivism Project

## Loading Data

```{r cars}
data = read.csv("NIJ_s_Recidivism_Challenge_Full_Dataset.csv")
```

## Pre-processing

data viz

```{r}
head(data,10)
```

```{r}
### Investigate missing values
data_cleaned <- data %>%
  mutate_all(~na_if(., ''))

# Column-wise
colSums(is.na(data_cleaned)) 

# Missing vals for female vs male
colSums(is.na(data_cleaned %>% filter(Gender == "M"))) 
#nrow(data_cleaned %>% filter(Gender == "F"))

# Row-wise
summary((rowSums(is.na(data_cleaned))))

# Investigate missing drug test data
#missing_drug_tests <- data_cleaned %>% filter(is.na(DrugTests_THC_Positive) | is.na(DrugTests_Meth_Positive))
#colSums(is.na(missing_drug_tests))

# Investigate missing Jobs data 
#missing_jobs <- data_cleaned %>% filter(is.na(Jobs_Per_Year))
#colSums(is.na(missing_jobs))

# For now just drop NAs
#data_cleaned <- data_cleaned %>%
#  na.omit() # removes almost half of our observations but data set still very large (note that this removes all entries for females)
```

```{r}
### Resolve missing data 

# Categorical vars
data_cleaned <- data_cleaned %>%
  mutate(Gang_Affiliated = ifelse(Gender == "F", "false", Gang_Affiliated),
         Supervision_Risk_Score_First = ifelse(is.na(Supervision_Risk_Score_First), "missing",   Supervision_Risk_Score_First),
         Supervision_Level_First = ifelse(is.na(Supervision_Level_First), "missing", Supervision_Level_First),
         Prison_Offense = ifelse(is.na(Prison_Offense), "missing", Prison_Offense))

```

```{r}

```

```{r}
# Remove 'DrugTest' vars (significant proportion is missing)
data_cleaned <- data_cleaned %>%
  na.omit() %>%# removes almost half of our observations but data set still very large (note that this removes all entries for females)
  select(-c(DrugTests_Cocaine_Positive, DrugTests_Meth_Positive, DrugTests_Other_Positive, Avg_Days_per_DrugTest,DrugTests_THC_Positive))
  
# Replace missing jobs data with values 0
data_cleaned <- data_cleaned %>%
  mutate(Percent_Days_Employed = ifelse(is.na(Percent_Days_Employed), 0, Percent_Days_Employed),
         Jobs_Per_Year = ifelse(is.na(Jobs_Per_Year), 0, Jobs_Per_Year))

colSums(is.na(data_cleaned)) 
```

```{r}
### Set up dependent var and covariate matrix
y <- data_cleaned %>%
  select(Recidivism_Within_3years) %>%
  mutate(Recidivism_Within_3years = ifelse(Recidivism_Within_3years == "true", 1, 0)) %>%
  as.matrix()%>%
  as.integer()

# 8423 observations correspond to a reoffence within three years
#sum(y$Recidivism_Within_3years)/length(y)

### Covariate matrix - removed supervision_risk_score as this is what we're trying to predict
X <- data_cleaned %>%
  select(-c(ID, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3, Training_Sample,Recidivism_Within_3years, Supervision_Risk_Score_First)) 

# dummy col 
#dummy_colnames <- X %>% select(-c(Avg_Days_per_DrugTest, DrugTests_THC_Positive,DrugTests_Cocaine_Positive, #DrugTests_Meth_Positive, DrugTests_Other_Positive, Percent_Days_Employed, Jobs_Per_Year)) %>%
#  names()
dummy_colnames <- X %>% select(-c(Percent_Days_Employed, Jobs_Per_Year)) %>%
  names()

X <- dummy_cols(X, select_columns = dummy_colnames, remove_first_dummy = TRUE, remove_selected_columns = TRUE)

X <- model.matrix(~., data = X)

```

## Models

### Regular logistic model

```{r}
glm.fit <- glm(y ~.,family=binomial(link='logit'), data = as.data.frame(X[,-1]))
#summary(glm.fit)
```

```{r}
## Store results
results <- glm.fit %>% 
  tidy() 

results[,2:ncol(results)] <- round(results[,2:ncol(results)], 4) 

results <- results %>%
  mutate(exp_estimate = exp(estimate))
```

```{r}
# Check in-sample fit
pred <- predict(glm.fit, newx = X[,-1])

assess.glmnet(pred, newy = y, family = "binomial")
```

```{r}
# Pseudo R2
round(pR2(glm.fit),2)
```

```{r}
### Assess out-of-sample performance

#Implement 10-fold Cross validation
kfoldCV.mle <- function(y , x, K = 10, seed = 1) {
  ## Perform K-fold cross-validation for logistic regression
  ## Input
  ## - y: response
  ## - x: data.frame with predictors, intercept should not be present
  ## - K: number of folds in K-fold cross-validation
  ## - seed: random number generator seed (optional)
  ## Output
  ## - pred: cross-validated predictions for y
  ## - ssr: residual sum of squares, sum((y-pred)^2)
  if (!missing(seed)) set.seed(seed)
  subset <- rep(1:K,ceiling(nrow(x)/K))[1:nrow(x)]
  subset <- sample(subset,size=nrow(x),replace=FALSE)
  pred <- double(nrow(x))
  if (ncol(x)>0) {
    for (k in 1:K) {
      sel <- subset==k 
      fit <- glmnet(x=x[!sel,,drop=FALSE], y=y[!sel], family="binomial", lambda=0)  
      pred[sel] <- predict(fit, newx=x[sel,,drop=FALSE])
    }
  } else {
    for (k in 1:K) {
      sel <- subset==k
      pred[sel] <- mean(y[!sel],na.rm=TRUE)
    }
  }
  return(list(pred=pred))
}

cv.mle = kfoldCV.mle(y = y, x= as.matrix(X[,-1]), K = 10, seed = 1)

# 10-fold cross-validated AUC
cv.mle.perf <-assess.glmnet(cv.mle$pred, newy = y, family = "binomial")
cv.mle.perf
```

```{r}
confusion.glmnet(cv.mle$pred, newy = y,family='binomial')
```

```{r}
plt<- plot(roc.glmnet(cv.mle$pred, newy = y,family='binomial'),type="s")+title(main = "MLE ROC") + text(0.8,0.1,paste("AUC=", round(cv.mle.perf$auc[1],5)))
```

### LASSO

```{r}
### LASSO Model
cv_lasso <- cv.glmnet(X, y, family="binomial", n_folds = 10, type.measure = 'auc')
plot(cv_lasso)
# In-sample AUC
paste("AUC: ", round(cv_lasso$cvm[cv_lasso$index[1]], 5))

#fit.lassoCV <-kfoldCV.lasso(y = y, x= as.matrix(X[,-1]),K=10,seed=1,criterion="cv")$pred


```

```{r}
# Non-zero covariates
paste('Non-zero covariates:',cv_lasso$nzero[cv_lasso$index[1]])
paste('Variable count pushed to zero =',sum(coef(cv_lasso,s='lambda.min') ==0))
paste("lambda=", round(cv_lasso$lambda.min,6))
```

```{r}
plt<-plot(cv_lasso$glmnet.fit, xvar='lambda')
```

### LASSO BIC

```{r}
fit.lassobic= lasso.bic(y = y, x= as.matrix(X[,-1]),extended = FALSE)
fit.lassoBICcv <-kfoldCV.lasso(y = y, x= as.matrix(X[,-1]), K = 10, seed = 1,criterion="bic")


```

```{r}
confusion.glmnet(fit.lassoBICcv$pred, newx = x[,-1], newy = y,family='binomial')
lassoBIC.perf<-assess.glmnet(fit.lassoBICcv$pred,newy=y,family='binomial')
cat('AUC Score = ',lassoBIC.perf$auc)

```

```{r}
lassoBIC.auc <- lassoBIC.perf$auc
lassoBIC.mise <- lassoBIC.perf$class
plt<-plot(roc.glmnet(fit.lassoBICcv$pred, newy = y,family='binomial'),type="s") + 
  title(main = "BIC ROC")+ text(0.8,0.2,paste("AUC=", round(lassoBIC.perf$auc,5)))+text(0.8,0.1,paste("lambda=", round(fit.lassobic$lambda.opt,5)))
```

```{r}
paste('Variable count pushed to 0 by BIC :',length(which(fit.lassobic$coef==0)))

```

```{r}
plot(coef(glm.fit),xlab='Index', ylab= "Beta for variable i",main='Coefficients',col = "red",pch=4)
points(coef(cv_lasso,s='lambda.min'),col = "blue",pch=2) 
points(fit.lassobic$coef,pch=1,col = 'darkorange',)
legend(x = "bottomright", legend = c('MLE','LASSO','BIC'),col=c("red", "blue",'darkorange'), pch=c(4,2,1),cex = 1)
```

```{r}
plot(roc.glmnet(cv.mle$pred, newy = y,family='binomial'),type="s",col='red') 
#lines(roc.glmnet(fit.lassoCV, newy = y,family='binomial',s=cv_lasso$lambda.min),col = 'blue')
lines(roc.glmnet(fit.lassoBICcv$pred, newy = y,family='binomial',s=fit.cvlasso$lambda.min),col='darkorange')
legend(x = "bottomleft", legend = c('MLE','BIC'),col=c("red",'darkorange'),pch = c(20,20),cex = 1)
title(main = 'ROC curve for different models')
```

### Hierarchical Models?

<https://stats.oarc.ucla.edu/r/dae/mixed-effects-logistic-regression/>

```{r}

```
